{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-07-07T06:42:49.066778Z","iopub.status.busy":"2024-07-07T06:42:49.066039Z","iopub.status.idle":"2024-07-07T06:42:54.358585Z","shell.execute_reply":"2024-07-07T06:42:54.357594Z","shell.execute_reply.started":"2024-07-07T06:42:49.066742Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/data1/home/nidhi2023/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]},{"name":"stdout","output_type":"stream","text":["Random Seed:  999\n"]}],"source":["#%matplotlib inline\n","import argparse\n","import os\n","import random\n","import torch\n","import torch.nn as nn\n","import torch.nn.parallel\n","import torch.optim as optim\n","import torch.utils.data\n","from torch.utils.data import Dataset,DataLoader\n","import torchvision.transforms as transforms\n","import torchvision.utils as vutils\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","from IPython.display import HTML\n","import pandas as pd\n","import h5py\n","from PIL import Image\n","from io import BytesIO\n","\n","\n","# Set random seed for reproducibility\n","manualSeed = 999\n","#manualSeed = random.randint(1, 10000) # use if you want new results\n","print(\"Random Seed: \", manualSeed)\n","random.seed(manualSeed)\n","torch.manual_seed(manualSeed)\n","torch.use_deterministic_algorithms(False) # Needed for reproducible results"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# !kaggle competitions download -c isic-2024-challenge"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-07-07T06:42:54.360817Z","iopub.status.busy":"2024-07-07T06:42:54.360323Z","iopub.status.idle":"2024-07-07T06:42:54.396088Z","shell.execute_reply":"2024-07-07T06:42:54.395094Z","shell.execute_reply.started":"2024-07-07T06:42:54.360789Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'cuda:1'"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# Setup device-agnostic code\n","device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n","device"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-07-07T06:42:54.397648Z","iopub.status.busy":"2024-07-07T06:42:54.397318Z","iopub.status.idle":"2024-07-07T06:43:03.679912Z","shell.execute_reply":"2024-07-07T06:43:03.679038Z","shell.execute_reply.started":"2024-07-07T06:42:54.397612Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>isic_id</th>\n","      <th>target</th>\n","      <th>patient_id</th>\n","      <th>age_approx</th>\n","      <th>sex</th>\n","      <th>anatom_site_general</th>\n","      <th>clin_size_long_diam_mm</th>\n","      <th>image_type</th>\n","      <th>tbp_tile_type</th>\n","      <th>tbp_lv_A</th>\n","      <th>...</th>\n","      <th>lesion_id</th>\n","      <th>iddx_full</th>\n","      <th>iddx_1</th>\n","      <th>iddx_2</th>\n","      <th>iddx_3</th>\n","      <th>iddx_4</th>\n","      <th>iddx_5</th>\n","      <th>mel_mitotic_index</th>\n","      <th>mel_thick_mm</th>\n","      <th>tbp_lv_dnn_lesion_confidence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>ISIC_0015670</td>\n","      <td>0</td>\n","      <td>IP_1235828</td>\n","      <td>60.0</td>\n","      <td>male</td>\n","      <td>lower extremity</td>\n","      <td>3.04</td>\n","      <td>TBP tile: close-up</td>\n","      <td>3D: white</td>\n","      <td>20.244422</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>Benign</td>\n","      <td>Benign</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>97.517282</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ISIC_0015845</td>\n","      <td>0</td>\n","      <td>IP_8170065</td>\n","      <td>60.0</td>\n","      <td>male</td>\n","      <td>head/neck</td>\n","      <td>1.10</td>\n","      <td>TBP tile: close-up</td>\n","      <td>3D: white</td>\n","      <td>31.712570</td>\n","      <td>...</td>\n","      <td>IL_6727506</td>\n","      <td>Benign</td>\n","      <td>Benign</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>3.141455</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>ISIC_0015864</td>\n","      <td>0</td>\n","      <td>IP_6724798</td>\n","      <td>60.0</td>\n","      <td>male</td>\n","      <td>posterior torso</td>\n","      <td>3.40</td>\n","      <td>TBP tile: close-up</td>\n","      <td>3D: XP</td>\n","      <td>22.575830</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>Benign</td>\n","      <td>Benign</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>99.804040</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>ISIC_0015902</td>\n","      <td>0</td>\n","      <td>IP_4111386</td>\n","      <td>65.0</td>\n","      <td>male</td>\n","      <td>anterior torso</td>\n","      <td>3.22</td>\n","      <td>TBP tile: close-up</td>\n","      <td>3D: XP</td>\n","      <td>14.242329</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>Benign</td>\n","      <td>Benign</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>99.989998</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ISIC_0024200</td>\n","      <td>0</td>\n","      <td>IP_8313778</td>\n","      <td>55.0</td>\n","      <td>male</td>\n","      <td>anterior torso</td>\n","      <td>2.73</td>\n","      <td>TBP tile: close-up</td>\n","      <td>3D: white</td>\n","      <td>24.725520</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>Benign</td>\n","      <td>Benign</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>70.442510</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows Ã— 55 columns</p>\n","</div>"],"text/plain":["        isic_id  target  patient_id  age_approx   sex anatom_site_general  \\\n","0  ISIC_0015670       0  IP_1235828        60.0  male     lower extremity   \n","1  ISIC_0015845       0  IP_8170065        60.0  male           head/neck   \n","2  ISIC_0015864       0  IP_6724798        60.0  male     posterior torso   \n","3  ISIC_0015902       0  IP_4111386        65.0  male      anterior torso   \n","4  ISIC_0024200       0  IP_8313778        55.0  male      anterior torso   \n","\n","   clin_size_long_diam_mm          image_type tbp_tile_type   tbp_lv_A  ...  \\\n","0                    3.04  TBP tile: close-up     3D: white  20.244422  ...   \n","1                    1.10  TBP tile: close-up     3D: white  31.712570  ...   \n","2                    3.40  TBP tile: close-up        3D: XP  22.575830  ...   \n","3                    3.22  TBP tile: close-up        3D: XP  14.242329  ...   \n","4                    2.73  TBP tile: close-up     3D: white  24.725520  ...   \n","\n","    lesion_id  iddx_full  iddx_1  iddx_2  iddx_3  iddx_4  iddx_5  \\\n","0         NaN     Benign  Benign     NaN     NaN     NaN     NaN   \n","1  IL_6727506     Benign  Benign     NaN     NaN     NaN     NaN   \n","2         NaN     Benign  Benign     NaN     NaN     NaN     NaN   \n","3         NaN     Benign  Benign     NaN     NaN     NaN     NaN   \n","4         NaN     Benign  Benign     NaN     NaN     NaN     NaN   \n","\n","   mel_mitotic_index  mel_thick_mm  tbp_lv_dnn_lesion_confidence  \n","0                NaN           NaN                     97.517282  \n","1                NaN           NaN                      3.141455  \n","2                NaN           NaN                     99.804040  \n","3                NaN           NaN                     99.989998  \n","4                NaN           NaN                     70.442510  \n","\n","[5 rows x 55 columns]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["train_matadata = pd.read_csv(\"train-metadata.csv\", low_memory=False)\n","train_matadata.head()"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-07-07T06:43:03.681434Z","iopub.status.busy":"2024-07-07T06:43:03.681122Z","iopub.status.idle":"2024-07-07T06:43:03.689696Z","shell.execute_reply":"2024-07-07T06:43:03.688700Z","shell.execute_reply.started":"2024-07-07T06:43:03.681407Z"},"trusted":true},"outputs":[],"source":["class ImageLoader(Dataset):\n","    def __init__(self, df, file_hdf, transform=None):\n","        self.df = df\n","        self.fp_hdf = h5py.File(file_hdf, mode=\"r\")\n","        self.isic_ids = df[df['target']!=1]['isic_id'].values\n","#         self.targets = df['target'].values\n","        self.transform = transform\n","        \n","    def __len__(self):\n","        return len(self.isic_ids)\n","    \n","    def __getitem__(self, index):\n","        isic_id = self.isic_ids[index]\n","        image = Image.open(BytesIO(self.fp_hdf[isic_id][()]))\n","#         target = self.targets[index]\n","#         image = torch.permute(image,(2,1,0))\n","        if self.transform:\n","            return (self.transform(image))\n","        else:\n","            return (image)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-07-07T06:43:03.692292Z","iopub.status.busy":"2024-07-07T06:43:03.691991Z","iopub.status.idle":"2024-07-07T06:43:03.869409Z","shell.execute_reply":"2024-07-07T06:43:03.868633Z","shell.execute_reply.started":"2024-07-07T06:43:03.692268Z"},"trusted":true},"outputs":[],"source":["train_transforms = transforms.Compose([\n","                               transforms.Resize(128),\n","                               transforms.CenterCrop(128),\n","                               transforms.ToTensor(),\n","                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","                           ])\n","\n","train_dataset_full = ImageLoader(train_matadata,\n","                      file_hdf = \"train-image.hdf5\",\n","                      transform=train_transforms\n","                     )\n","\n","train_size = int(0.95 * len(train_dataset_full))\n","test_size = len(train_dataset_full) - train_size\n","train_dataset, test_dataset = torch.utils.data.random_split(train_dataset_full, [train_size, test_size])"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-07-07T06:43:03.871127Z","iopub.status.busy":"2024-07-07T06:43:03.870834Z","iopub.status.idle":"2024-07-07T06:43:03.875385Z","shell.execute_reply":"2024-07-07T06:43:03.874524Z","shell.execute_reply.started":"2024-07-07T06:43:03.871101Z"},"trusted":true},"outputs":[],"source":["\n","import numpy as np\n","\n","\n","import torch\n","import torchvision\n","from torch import nn\n","from torch import autograd\n","from torch import optim"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-07-07T06:43:03.877320Z","iopub.status.busy":"2024-07-07T06:43:03.876800Z","iopub.status.idle":"2024-07-07T06:43:04.074595Z","shell.execute_reply":"2024-07-07T06:43:04.073700Z","shell.execute_reply.started":"2024-07-07T06:43:03.877289Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Generator(\n","  (preprocess): Sequential(\n","    (0): Linear(in_features=128, out_features=512, bias=True)\n","    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU(inplace=True)\n","  )\n","  (block1): Sequential(\n","    (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2))\n","    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU(inplace=True)\n","    (3): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2))\n","    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (5): ReLU(inplace=True)\n","    (6): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2))\n","    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (8): ReLU(inplace=True)\n","    (9): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2))\n","    (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (11): ReLU(inplace=True)\n","    (12): ConvTranspose2d(128, 32, kernel_size=(2, 2), stride=(2, 2))\n","    (13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (14): ReLU(inplace=True)\n","  )\n","  (deconv_out): ConvTranspose2d(32, 3, kernel_size=(2, 2), stride=(2, 2))\n","  (tanh): Tanh()\n",")\n","Discriminator(\n","  (main): Sequential(\n","    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","    (1): LeakyReLU(negative_slope=0.01)\n","    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","    (3): LeakyReLU(negative_slope=0.01)\n","    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","    (5): LeakyReLU(negative_slope=0.01)\n","    (6): Conv2d(128, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","    (7): LeakyReLU(negative_slope=0.01)\n","    (8): Conv2d(192, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","    (9): LeakyReLU(negative_slope=0.01)\n","    (10): Conv2d(96, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","    (11): LeakyReLU(negative_slope=0.01)\n","  )\n","  (linear): Linear(in_features=128, out_features=1, bias=True)\n",")\n"]}],"source":["MODE = 'wgan-gp' # Valid options are dcgan, wgan, or wgan-gp\n","DIM = 32 # This overfits substantially; you're probably better off with 64\n","LAMBDA = 10 # Gradient penalty lambda hyperparameter\n","CRITIC_ITERS = 5 # How many critic iterations per generator iteration\n","BATCH_SIZE = 128 # Batch size\n","ITERS = 200000 # How many generator iterations to train for\n","OUTPUT_DIM = 3072 # Number of pixels in CIFAR10 (3*32*32)\n","\n","\n","class Generator(nn.Module):\n","    def __init__(self):\n","        super(Generator, self).__init__()\n","        preprocess = nn.Sequential(\n","            nn.Linear(128, 2 * 2 * 4 * DIM),\n","            nn.BatchNorm1d(2 * 2 * 4 * DIM),\n","            nn.ReLU(True),\n","        )\n","\n","        block1 = nn.Sequential(\n","            nn.ConvTranspose2d(4 * DIM, 4 * DIM, 2, stride=2),\n","            nn.BatchNorm2d(4 * DIM),\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(4 * DIM, 4 * DIM, 2, stride=2),\n","            nn.BatchNorm2d(4 * DIM),\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(4 * DIM, 4 * DIM, 2, stride=2),\n","            nn.BatchNorm2d(4 * DIM),\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(4 * DIM, 4 * DIM, 2, stride=2),\n","            nn.BatchNorm2d(4 * DIM),\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(4 * DIM, DIM, 2, stride=2),\n","            nn.BatchNorm2d(DIM),\n","            nn.ReLU(True),\n","        )\n","        deconv_out = nn.ConvTranspose2d(DIM, 3, 2, stride=2)\n","\n","        self.preprocess = preprocess\n","        self.block1 = block1\n","        self.deconv_out = deconv_out\n","        self.tanh = nn.Tanh()\n","\n","    def forward(self, input):\n","#         print(input.shape)\n","        output = self.preprocess(input)\n","#         print(output.shape)\n","        output = output.view(-1, 4 * DIM, 2, 2)\n","        output = self.block1(output)\n","#         print(output.shape)\n","        output = self.deconv_out(output)\n","#         print(output.shape)\n","        output = self.tanh(output)\n","        return output.view(-1, 3, 128, 128)\n","\n","\n","class Discriminator(nn.Module):\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","        self.main = nn.Sequential(\n","            nn.Conv2d(3, DIM, 3, 2, padding=1),\n","            nn.LeakyReLU(),\n","            nn.Conv2d(DIM, 2 * DIM, 3, 2, padding=1),\n","            nn.LeakyReLU(),\n","            nn.Conv2d(2 * DIM, 4 * DIM, 3, 2, padding=1),\n","            nn.LeakyReLU(),\n","            nn.Conv2d(4 * DIM, 6 * DIM, 3, 2, padding=1),\n","            nn.LeakyReLU(),\n","            nn.Conv2d(6 * DIM, 3 * DIM, 3, 2, padding=1),\n","            nn.LeakyReLU(),\n","            nn.Conv2d(3 * DIM, DIM, 3, 2, padding=1),\n","            nn.LeakyReLU(),\n","        )\n","\n","        self.linear = nn.Linear(2*2*DIM, 1)\n","\n","    def forward(self, input):\n","        output = self.main(input)\n","        output = output.view(-1, 2*2*DIM)\n","        output = self.linear(output)\n","        return output\n","\n","netG = Generator()\n","netD = Discriminator()\n","print(netG)\n","print(netD)\n","\n","use_cuda = torch.cuda.is_available()\n","if use_cuda:\n","    gpu = 0\n","if use_cuda:\n","    netD = netD.cuda(gpu)\n","    netG = netG.cuda(gpu)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-07-07T06:43:04.076260Z","iopub.status.busy":"2024-07-07T06:43:04.075886Z","iopub.status.idle":"2024-07-07T06:43:04.102446Z","shell.execute_reply":"2024-07-07T06:43:04.101741Z","shell.execute_reply.started":"2024-07-07T06:43:04.076225Z"},"trusted":true},"outputs":[],"source":["one = torch.tensor(1, dtype=torch.float)\n","mone = one * -1\n","if use_cuda:\n","    one = one.cuda(gpu)\n","    mone = mone.cuda(gpu)\n","\n","optimizerD = optim.Adam(netD.parameters(), lr=1e-4, betas=(0.5, 0.9))\n","optimizerG = optim.Adam(netG.parameters(), lr=1e-4, betas=(0.5, 0.9))\n","\n","def calc_gradient_penalty(netD, real_data, fake_data):\n","#     print \"real_data: \", real_data.size(), fake_data.size()\n","#     print(real_data.nelement())\n","    alpha = torch.rand(BATCH_SIZE, 1)\n","    alpha = alpha.expand(BATCH_SIZE, int(real_data.nelement()/BATCH_SIZE)).contiguous().view(BATCH_SIZE, 3, 128, 128)\n","    alpha = alpha.cuda(gpu) if use_cuda else alpha\n","\n","    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n","\n","    if use_cuda:\n","        interpolates = interpolates.cuda(gpu)\n","    interpolates = autograd.Variable(interpolates, requires_grad=True)\n","\n","    disc_interpolates = netD(interpolates)\n","\n","    gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n","                              grad_outputs=torch.ones(disc_interpolates.size()).cuda(gpu) if use_cuda else torch.ones(\n","                                  disc_interpolates.size()),\n","                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n","    gradients = gradients.view(gradients.size(0), -1)\n","\n","    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * LAMBDA\n","    return gradient_penalty\n","\n","# For generating samples\n","def generate_image(frame, netG):\n","    fixed_noise_128 = torch.randn(128, 128)\n","    if use_cuda:\n","        fixed_noise_128 = fixed_noise_128.cuda(gpu)\n","    with torch.no_grad():\n","        noisev = autograd.Variable(fixed_noise_128)#, volatile=True)\n","        samples = netG(noisev)\n","    samples = samples.view(-1, 3, 128, 128)\n","    samples = samples.mul(0.5).add(0.5)\n","    # samples = samples.cpu().data.numpy()\n","    torchvision.utils.save_image(samples,'./samples/samples_{}.jpg'.format(frame))\n","    # save_images(samples, './samples/samples_{}.jpg'.format(frame))\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["class InfiniteSampler(torch.utils.data.Sampler):\n","    def __init__(self, dataset, num_replicas=1, shuffle=True, seed=0):\n","        assert len(dataset) > 0\n","        super().__init__(dataset)\n","        self.dataset = dataset\n","        self.num_replicas = num_replicas\n","        self.shuffle = shuffle\n","        self.seed = seed\n","\n","    def __iter__(self):\n","        order = np.arange(len(self.dataset))\n","        rnd = None\n","        window = 0\n","        if self.shuffle:\n","            rnd = np.random.RandomState(self.seed)\n","            rnd.shuffle(order)\n","\n","        idx = 0\n","        while True:\n","            i = idx % order.size\n","            yield order[i]\n","            idx += 1"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-07-07T06:57:25.487014Z","iopub.status.busy":"2024-07-07T06:57:25.486300Z","iopub.status.idle":"2024-07-07T06:57:25.878880Z","shell.execute_reply":"2024-07-07T06:57:25.877070Z","shell.execute_reply.started":"2024-07-07T06:57:25.486981Z"},"trusted":true},"outputs":[],"source":["train_dataset[0].shape\n","training_set_sampler = InfiniteSampler(dataset=train_dataset)\n","\n","dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE,\n","                                         sampler=training_set_sampler, num_workers=4,drop_last=True)\n","dev_gen = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=4,drop_last=True)\n","\n","\n","\n","# Decide which device we want to run on\n","device = torch.device(\"cuda\" if (torch.cuda.is_available()) else \"cpu\")\n","\n","ds = iter(dataloader)\n","\n","\n","# next(ds).shape"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-07-07T06:57:28.056029Z","iopub.status.busy":"2024-07-07T06:57:28.055573Z","iopub.status.idle":"2024-07-07T06:57:28.079119Z","shell.execute_reply":"2024-07-07T06:57:28.078217Z","shell.execute_reply.started":"2024-07-07T06:57:28.055992Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","\n","import matplotlib\n","matplotlib.use('Agg')\n","import matplotlib.pyplot as plt\n","\n","import collections\n","import time\n","import pickle\n","\n","_since_beginning = collections.defaultdict(lambda: {})\n","_since_last_flush = collections.defaultdict(lambda: {})\n","\n","_iter = [0]\n","def tick():\n","    _iter[0] += 1\n","\n","def plot(name, value):\n","    _since_last_flush[name][_iter[0]] = value\n","\n","def flush():\n","    prints = []\n","\n","    for name, vals in _since_last_flush.items():\n","        prints.append(\"{}\\t{}\".format(name, np.mean(list(vals.values()))))\n","        _since_beginning[name] = vals\n","        x_vals = np.sort(list(_since_beginning[name].keys()))\n","        y_vals = [_since_beginning[name][x] for x in x_vals]\n","\n","        plt.clf()\n","        plt.plot(x_vals, y_vals)\n","        plt.xlabel('iteration')\n","        plt.ylabel(name)\n","        plt.savefig(name.replace(' ', '_')+'.jpg')\n","\n","    print (\"iter {}\\t{}\".format(_iter[0], \"\\t\".join(prints)))\n","    # _since_last_flush.clear()\n","\n","    with open('log.pkl', 'wb') as f:\n","        pickle.dump(dict(_since_beginning), f, pickle.HIGHEST_PROTOCOL)\n","        \n","\"\"\"\n","Image grid saver, based on color_grid_vis from github.com/Newmu\n","\"\"\"\n","\n","import numpy as np\n","import scipy.misc\n","# from keras.preprocessing.image import save_img\n","# from scipy.misc import imsave\n","\n","def save_images(X, save_path):\n","    # [0, 1] -> [0,255]\n","    if isinstance(X.flatten()[0], np.floating):\n","        X = (255.99*X).astype('uint8')\n","\n","    n_samples = X.shape[0]\n","    rows = int(np.sqrt(n_samples))\n","    while n_samples % rows != 0:\n","        rows -= 1\n","\n","    nh, nw = rows, n_samples/rows\n","\n","    if X.ndim == 2:\n","        X = np.reshape(X, (X.shape[0], int(np.sqrt(X.shape[1])), int(np.sqrt(X.shape[1]))))\n","\n","    if X.ndim == 4:\n","        # BCHW -> BHWC\n","        X = X.transpose(0,2,3,1)\n","        h, w = X[0].shape[:2]\n","        img = np.zeros((int(h*nh), int(w*nw), 3))\n","    elif X.ndim == 3:\n","        h, w = X[0].shape[:2]\n","        img = np.zeros((h*nh, w*nw))\n","#     print(len(X),h,w)\n","#     print(img.shape)\n","    for n, x in enumerate(X):\n","        j = n//nw\n","        i = n%nw\n","#         print(j*h,i*w)\n","#         print(img[int(j*h):int(j*h+h), int(i*w):int(i*w+w)].shape)\n","        img[int(j*h):int(j*h+h), int(i*w):int(i*w+w)] = x\n","\n","    torchvision.utils.save_image(img,save_path)\n","    \n"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-07-07T07:47:18.032349Z","iopub.status.busy":"2024-07-07T07:47:18.031945Z","iopub.status.idle":"2024-07-07T07:47:18.133800Z","shell.execute_reply":"2024-07-07T07:47:18.132512Z","shell.execute_reply.started":"2024-07-07T07:47:18.032318Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["3.992314338684082\n","iter 0\t./tmp/train disc cost\t9.938619613647461\t./tmp/time\t3.9922008514404297\t./tmp/train gen cost\t0.05161835253238678\t./tmp/wasserstein distance\t0.008324846625328064\t./tmp/dev disc cost\t0.03966846689581871\n","1348.1284651756287\n","iter 2000\t./tmp/train disc cost\t-47.411014556884766\t./tmp/time\t698.497243160489\t./tmp/train gen cost\t-36.322757720947266\t./tmp/wasserstein distance\t70.22871398925781\t./tmp/dev disc cost\t-22.752971649169922\n","2571.138253211975\n","iter 4000\t./tmp/train disc cost\t-29.132654190063477\t./tmp/time\t1326.69594285304\t./tmp/train gen cost\t-24.03594398498535\t./tmp/wasserstein distance\t41.74999237060547\t./tmp/dev disc cost\t-25.417001724243164\n","3972.811932325363\n","iter 6000\t./tmp/train disc cost\t-22.296045303344727\t./tmp/time\t1967.0481881867288\t./tmp/train gen cost\t-15.211920738220215\t./tmp/wasserstein distance\t31.22220230102539\t./tmp/dev disc cost\t-21.40266227722168\n","5374.446241855621\n","iter 8000\t./tmp/train disc cost\t-18.542421340942383\t./tmp/time\t2639.2245919999145\t./tmp/train gen cost\t-10.586124420166016\t./tmp/wasserstein distance\t25.535398483276367\t./tmp/dev disc cost\t-18.709579467773438\n","6816.227844238281\n","iter 10000\t./tmp/train disc cost\t-16.123746871948242\t./tmp/time\t3337.6630238722637\t./tmp/train gen cost\t-7.068153381347656\t./tmp/wasserstein distance\t21.937480926513672\t./tmp/dev disc cost\t-15.289359092712402\n","8157.76411151886\n","iter 12000\t./tmp/train disc cost\t-14.418783187866211\t./tmp/time\t4033.5529149142653\t./tmp/train gen cost\t-4.608959197998047\t./tmp/wasserstein distance\t19.425493240356445\t./tmp/dev disc cost\t-13.144497871398926\n","9179.133248090744\n","iter 14000\t./tmp/train disc cost\t-13.140547752380371\t./tmp/time\t4695.974422125738\t./tmp/train gen cost\t-2.819603204727173\t./tmp/wasserstein distance\t17.55597496032715\t./tmp/dev disc cost\t-9.798811912536621\n","10604.401275396347\n","iter 16000\t./tmp/train disc cost\t-12.133523941040039\t./tmp/time\t5348.10207779787\t./tmp/train gen cost\t-1.6570833921432495\t./tmp/wasserstein distance\t16.09260368347168\t./tmp/dev disc cost\t-9.580026626586914\n","11984.286234617233\n","iter 18000\t./tmp/train disc cost\t-11.306875228881836\t./tmp/time\t6007.374943206419\t./tmp/train gen cost\t-0.9432489275932312\t./tmp/wasserstein distance\t14.898988723754883\t./tmp/dev disc cost\t-8.404043197631836\n","13483.883544445038\n","iter 20000\t./tmp/train disc cost\t-10.620211601257324\t./tmp/time\t6680.09907723701\t./tmp/train gen cost\t-0.532711386680603\t./tmp/wasserstein distance\t13.913877487182617\t./tmp/dev disc cost\t-7.156369209289551\n","14761.709023237228\n","iter 22000\t./tmp/train disc cost\t-10.03469181060791\t./tmp/time\t7360.522477705876\t./tmp/train gen cost\t-0.3400284945964813\t./tmp/wasserstein distance\t13.079513549804688\t./tmp/dev disc cost\t-7.363515377044678\n","15915.93708062172\n","iter 24000\t./tmp/train disc cost\t-9.528693199157715\t./tmp/time\t8025.402039646998\t./tmp/train gen cost\t-0.29160258173942566\t./tmp/wasserstein distance\t12.363615036010742\t./tmp/dev disc cost\t-7.522289752960205\n"]}],"source":["start_time = time.time()\n","for iteration in range(ITERS):\n","    \n","    ############################\n","    # (1) Update D network\n","    ###########################\n","    for p in netD.parameters():  # reset requires_grad\n","        p.requires_grad = True  # they are set to False below in netG update\n","        \n","    for i in range(CRITIC_ITERS):\n","        _data = next(ds)\n","        netD.zero_grad()\n","\n","        # train with real\n","#         _data = _data.reshape(BATCH_SIZE, 3, 128, 128).permute(0, 2, 3, 1)\n","#         real_data = torch.stack([item for item in _data])\n","        real_data = _data\n","\n","        if use_cuda:\n","            real_data = real_data.cuda(gpu)\n","        \n","        D_real = netD(real_data)\n","        D_real = D_real.mean()\n","        D_real.backward(mone)\n","\n","        # train with fake\n","        noise = torch.randn(BATCH_SIZE, 128)\n","        if use_cuda:\n","            noise = noise.cuda(gpu)\n","            \n","        fake = netG(noise)\n","        D_fake = netD(fake)\n","        D_fake = D_fake.mean()\n","        D_fake.backward(one)\n","#         print(real_data_v.shape)\n","\n","        # train with gradient penalty\n","        gradient_penalty = calc_gradient_penalty(netD, real_data.data, fake.data)\n","        gradient_penalty.backward()\n","\n","        D_cost = D_fake - D_real + gradient_penalty\n","        Wasserstein_D = D_real - D_fake\n","        optimizerD.step()\n","\n","    ############################\n","    # (2) Update G network\n","    ###########################\n","    for p in netD.parameters():\n","        p.requires_grad = False  # to avoid computation\n","    netG.zero_grad()\n","\n","    noise = torch.randn(BATCH_SIZE, 128)\n","    if use_cuda:\n","        noise = noise.cuda(gpu)\n","    fake = netG(noise)\n","    G = netD(fake)\n","    G = G.mean()\n","    G.backward(mone)\n","    G_cost = -G\n","    optimizerG.step()\n","\n","    # Write logs and save samples\n","    plot('./tmp/train disc cost', D_cost.cpu().data.numpy())\n","    plot('./tmp/time', time.time() - start_time)\n","    plot('./tmp/train gen cost', G_cost.cpu().data.numpy())\n","    plot('./tmp/wasserstein distance', Wasserstein_D.cpu().data.numpy())\n","\n","\n","    # Calculate dev loss and generate samples every 100 iters\n","    if iteration % 2000 == 0:\n","        print(time.time()-start_time)\n","        dev_disc_costs = []\n","        with torch.no_grad():\n","            for images in dev_gen:\n","                \n","                imgs = images\n","                # imgs = preprocess(images)\n","                if use_cuda:\n","                    imgs = imgs.cuda(gpu)\n","                imgs_v = autograd.Variable(imgs)\n","\n","                D = netD(imgs_v)\n","                _dev_disc_cost = -D.mean().cpu().data.numpy()\n","                dev_disc_costs.append(_dev_disc_cost)\n","            plot('./tmp/dev disc cost', np.mean(dev_disc_costs))\n","\n","            generate_image(iteration, netG)\n","    \n","    if (iteration % 4000 == 0):\n","        torch.save(netG.state_dict(), f'trained_nets/gen_{iteration}.pt')\n","        torch.save(netD.state_dict(), f'trained_nets/disc_{iteration}.pt')\n","    # Save logs every 100 iters\n","    if (iteration % 2000 == 0):\n","        flush()\n","    tick()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":8940774,"sourceId":63056,"sourceType":"competition"}],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":4}
